{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Support Vector Machines\n",
    "\n",
    "1. Support Vector Machines: Introduction & Motivation\n",
    "1. SVM Concepts\n",
    "1. Linear SVMs\n",
    "1. Nonlinear SVMs\n",
    "1. Applications of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Support Vector Machines: Introduction & Motivation\n",
    "\n",
    "Support Vector Machines (SVMs) are among the most important tools you will learn about in your study of machine learning. Although there are cases wherein SVMs are not necessarily the best choice, they can do many things that other models do. In some cases, SVMs are superior in performance to more sophisticated tools. \n",
    "\n",
    "The SVM is by definition a *Classifier* and discriminative model. At the most basic level, it is a **binary** classifier, meaning it distinguishes between two classes; think (+) or (-). More advanced SVMs can discriminate between many classes.\n",
    "\n",
    "Because of the intellectual importance of SVMs to a foundation in machine learning, we shall motivate and discuss them in some detail. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###SVMs: Motivation\n",
    "\n",
    "In your previous classwork, you observed the use of Lasso regression against an essentially binary dataset. Other forms of regularized regression reduced the emphasis of the regression on the x-value, but Lasso eliminated dependence on the x-value completely. Suppose, that instead of a regression, you were more worried about finding an **optimal boundary of discrimination** between two classes within the sample space, rather than trying to predict the next point. \n",
    "\n",
    "We shall define this boundary as follows:\n",
    "\n",
    "**Given an n-dimensional feature space, the optimal boundary is that n-dimensional hyperplane that maximizes distance between the two classes. This in turn, means we seek to optimize the distance between a training example of a given class and the plane itself.**\n",
    "\n",
    "\n",
    "###SVMs: Derivation\n",
    "\n",
    "Instead of projecting output variables into the feature space as we normally do, we are going to define the general equation of a (hyper)plane in the n-dimensional feature space (in two dimensions a plane is a line):\n",
    "\n",
    "$$\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_p = z$$\n",
    "\n",
    "$$\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x} = z$$\n",
    "\n",
    "Our plane is going to divide the two classes in hyperspace. There are an infinite number of ways of choosing the different coefficients to represent the plane, but in this case we are just going to make it as simple as possible:\n",
    "\n",
    "$$ |\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x}| = 1 $$\n",
    "\n",
    "In this case $\\textbf{x}$ represents those training examples closest to the hyperplane. This is an important element of what we are doing, because ideally we want to find the plane that maximizes distance between the positive and negative training examples. \n",
    "\n",
    "###Distance between a point and a plane\n",
    "\n",
    "Now we use vector math to calculate the distance between any point $x$ (a vector in p-dimensional space) and any plane. To illustrate this, take a plane z in 3 dimensions:\n",
    "\n",
    "$$ax_0+bx_1+cx_2+z=0$$\n",
    "\n",
    "The normal vector to the plane is:\n",
    "\n",
    "$$\\textbf{v} = \\left| \\begin{array}{c}\n",
    "a  \\\\\n",
    "b  \\\\\n",
    "c  \\end{array} \\right|$$\n",
    "\n",
    "\n",
    "Given a point $\\textbf{u} = (u_0,u_1,u_2)$, a vector $\\textbf{w}$ from a point $\\textbf{x}$ on the plane to $\\textbf{u}$ is given by:\n",
    "\n",
    "$$\\textbf{w} = \\textbf{u}-\\textbf{x} = - \\left| \\begin{array}{c}\n",
    "x_0-u_0  \\\\\n",
    "x_1-u_1  \\\\\n",
    "x_2-u_2  \\end{array} \\right|$$\n",
    "\n",
    "Now we can take the projection of $\\textbf{w}$ onto $\\textbf{v}$ to give the distance from $\\textbf{u}$:\n",
    "\n",
    "$$D=|proj_{\\textbf{v}}\\textbf{w}|$$\n",
    "\n",
    "$$D=\\frac{|\\textbf{v}\\cdot\\textbf{w}|}{|\\textbf{v}|}$$\n",
    "\n",
    "\n",
    "We have to work this equation to get the result we want:\n",
    "\n",
    "$$D = \\frac{|a(x_0-u_0)+b(x_1-u_1)+c(x_2-u_2)|}{|\\textbf{v}|}$$\n",
    "\n",
    "$$D = \\frac{|ax_0-au_0+bx_1-bu_1+cx_2-cu_2|}{|\\textbf{v}|}$$\n",
    "\n",
    "$$D = \\frac{|-z-au_0-bu_1-cu_2|}{|\\textbf{v}|}$$\n",
    "\n",
    "$$D = \\frac{au_0+bu_1+cu_2+z}{|\\textbf{v}|}$$\n",
    "\n",
    "###Definition of the SVM equations\n",
    "\n",
    "Extending these definitions to our discussion of distance from the hyperplane, we can simply compute the distance between the support vector $\\textbf{x}$ and the special hyperplane as:\n",
    "\n",
    "$$D_{hyperplane} = \\frac{|\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x}|}{\\|\\beta\\|}$$\n",
    "\n",
    "Extending it to our generalization, we can set the distance to the support vectors as:\n",
    "\n",
    "$$D_{hyperplane} = \\frac{|\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x}|}{\\|\\beta\\|} = \\frac{1}{\\|\\beta\\|}$$\n",
    "\n",
    "The total **margin** between the plane on both sides of the divide is given by **M**:\n",
    "\n",
    "$$M = \\frac{2}{\\|\\beta\\|}$$\n",
    "\n",
    "With an SVM, we seek to **maximize M**.\n",
    "\n",
    "###Solution to the SVM\n",
    "\n",
    "####Part 1\n",
    "\n",
    "Recall that we were discussing classifying the data into the (+) and (-) group. Thus we consider these classification states as output variables $\\textbf{y}$ to our method. Going back to our equation, we can write that \n",
    "\n",
    "$$\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x} = 1$$\n",
    "\n",
    "For the positive class, and\n",
    "\n",
    "$$\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x} = -1$$\n",
    "\n",
    "For the negative class, again recalling that the $y$ are either $+1$ or $-1$. \n",
    "\n",
    "Also recall that the plane includes all data point solutions above the plane in the positive class. All solutions below the plane are in the negative class, therefore;\n",
    "\n",
    "$$\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x} \\geq 1$$\n",
    "\n",
    "and also\n",
    "\n",
    "$$\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x} \\leq -1$$\n",
    "\n",
    "Thinking of the $y_i$ as multipliers by $+1$ or $-1$, we can write:\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta^{T}x_{i}) \\geq 1$$\n",
    "\n",
    "for all datapoints $i=1,2, \\cdots , m$\n",
    "\n",
    "####Part 2\n",
    "\n",
    "It so turns out that the above equation is miserably difficult to solve directly. This is because it involves "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SVM Concepts\n",
    "\n",
    "###Hypothesis\n",
    "\n",
    "The variables can be most clearly classified in terms of a plane separating them.\n",
    "\n",
    "**We can define the plane in terms of the training points closest to the plane. These are points are called *support vectors***.\n",
    "\n",
    "The equation of the plane is given as \n",
    "\n",
    "$$|\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x}|=1$$\n",
    "\n",
    "Where the $\\textbf{x}$ are the **support vectors**.\n",
    "\n",
    "\n",
    "###Cost Function\n",
    "\n",
    "Recakk\n",
    "\n",
    "###Optimization\n",
    "\n",
    "\n",
    "###Reasoning\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
