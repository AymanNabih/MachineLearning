# Recommender Systems
Recommenders have been popularized by Netflix's [$1,000,000 prize](http://www.netflixprize.com/). We've all experienced recommenders. When you go onto Netflix, Amazon or basically any other site where you can buy things, you will see a list of personalized recommendations.

How do they come up with recommendations? One way is a *content based recommender*. This uses the description of the item about the items and user preferences to make recommendations.

Today we will be talking about *collaborative filtering*. The recommendations are based on your and other users' buying and viewing history. In simple terms, you find items that are similar to items you've liked. Items are dubbed similar if the same people like both of them.


## Recommender Feature Matrix
Our feature matrix will consist of every user's rating of every item. Note that this is going to be a very sparse feature matrix. A lot of these values are unknown since most users have only rated a small percentage of the items.

It'll look something like this. The ratings are from 1 to 5 and -1 means the user has not rated the given item.

|            | item 1 | item 2 | item 3 | ... |
| :--------- | -----: | -----: | -----: | --- |
| **user 1** |      4 |     -1 |      2 | ... |
| **user 2** |     -1 |      5 |     -1 | ... |
| **user 3** |      4 |      3 |      2 | ... |
| **...**    |    ... |    ... |    ... | ... |


## Item-item similarity
We will be focusing on *item-item recommenders*. You can also compute similarity between two users in the same way.

A big reason to use item-item recommenders over user-user recommenders is the amount of computation. It's often the case that we have a limited number of items (say 10,000) and *a lot* of users (say 100,000). In this example, if we do item-item similarity, we have 10,000 * 10,000 = 100,000,000 computations. If we do user-user similarity, we have 100,000 * 100,000 = 10,000,000,000 computations!


## Computing Similarity
There are a few metrics used to determine the similarity of two items. Each similarity metric will range from 0 to 1, where 1 means identitcal and 0 means completely different.

We've seen *euclidean distance* and *cosine similarity* in the context of [k Nearest Neighbors](https://github.com/zipfian/non-parametric-learners/blob/master/lecture.md).

#### Euclidean Distance
Euclidean distance is the most intuitive of the distance metrics.

![euclidean distance](images/euclidean_distance.png)

A euclidean distance of 0 means the items are identical, and the euclidean distance is unbounded. So to get the values in the range of 0 to 1, we calculate the similarity as follows:

![euclidean similarity](images/euclidean_similarity.png)

#### Pearson Correlation
The Pearson Correlation tells us how similar to vectors are. Note that it measures how far the values are from the mean. 

![pearson correlation](images/pearson_correlation.png)

This is also the normalized covariance (in numpy, you can get it with [corrcoeff](http://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html#numpy.corrcoef)).

Again, we would like our similarity to range from 0 to 1 (1 being completely similar). The Pearson Correlation ranges from -1 to 1, so we do the following normalization.

![pearson similarity](images/pearson_similarity.png)

One advantage of the pearson correlation is that if you're measuring the similarity of two users, it isn't sensitive to a user who consistently rates low or high. Say user 1 rates three items: 5, 5, 3 and user 2 rates three items: 3, 3, 1. The similarity of these two users will be 1 (totally similar).


#### Cosine Similarity
The cosine similarity is a measure of the angle between two vectors. It's computed as follows.

![cosine similarity](images/cosine_similarity.png)

Again, we want to scale this so that the range is from 0 to 1. The cosine ranges from -1 (disimilar) to 1 (similar). So we can scale as follows.

![cosine similarity](images/cosine_similarity_scaled.png)


#### Jaccard Similarity
The Jaccard Similarity is a measure of the similarity of two sets. In this case, we would like to measure if two items have been rated by the same users. The Jaccard Similarity is useful when we don't have ratings, just a boolean value. For example, whether or not the user watched the movie, bought the product, etc.

![jaccard similarity](images/jaccard_similarity.png)

Note that this similarity metric already ranges from 0 (disimilar) to 1 (similar).


## Similarity Matrix
Using one of the similarity metrics, we compute a *similarity matrix*. If we have *m* items, this is an *m* by *m* matrix containing values from 0 to 1 giving the similarity of each item to each other item. Note that it will be symmetrical, since the similarity of item *x* to item *y* is the same as the similarity of item *y* to item *x*.

|            | item 1 | item 2 | item 3 | ... |
| :--------- | -----: | -----: | -----: | --- |
| **item 1** |      1 |    0.3 |    0.2 | ... |
| **item 2** |    0.3 |      1 |    0.7 | ... |
| **item 3** |    0.2 |    0.7 |      1 | ... |
| **...**    |    ... |    ... |    ... | ... |


## Predictions
Remember our goal was to give recommendations to users of movies they should watch (or products they should buy). So how do we get this from our similarity matrix?

Say user *u* hasn't rated item *i*. We would like to predict the rating that this user would give this item. We can think of this as a weighted average of the user's ratings of other items. Items that are more similar will get a higher weighting than items that are less similar.

![rating prediction](images/rating_prediction.png)

We calculate these predicted ratings for all the items that the user has not rated and order them by the predicted ratings. This is the order in which we would recommend items.


## Evaluating a Recommender
Recommenders are inherently hard to evaluate. In practice, we would launch the recommender with an A/B test and see if it leads to more conversions.

Beyond that, there isn't standard of how to evaluate a recommender. One way is to do a train test split. Build the feature matrix based only on the training set. Get the predicted ratings for the test set and calculate the root mean squared error (RMSE). You can use other cross validation methods like KFold Cross Validation as well. This is not a perfect measure, since it only considers how well you do on rating items that user has rated, which is a biased set.


## Issues facing recommenders

#### The Cold Start Problem
A collaborative filter doesn't work if you haven't rated any items yet. To get around this, a lot of recommenders will force users to rate some items to get started. If you've ever used Netflix, you'll notice that when you start it asks you to rate a few movies before it gives you any recommendations. This is to overcome the *cold start problem*.

#### Data sparsity
It's very common for most users to only have only rated a very small percentage of the items. This makes it difficult for the collaborative filter to work since many pairs of items won't have a lot of users who rated both of them.

We can deal with this by using matrix factorization and other dimensionality reduction techniques.