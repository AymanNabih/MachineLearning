## Intro to AdaBoost Classifier
This morning we have encountered `AdaBoostRegressor` and its gradient
descent variant, `GradientBoostingRegressor`. The base form of AdaBoost was
introduced in 1995 as an ensemble classifer, `AdaBoostClassifier`.
Understanding `AdaBoostClassifier` is regarded as the defacto
introduction to the world of seemingly endless variants of boosting algorithms
([refs](readings)). To gain a more entrenched understanding of boosting
in general, I would recommend [this](readings/explaining_boosting.pdf).

Your code should exactly implement this pseudocode:

![adaboost](images/adaboost_algorithm.png)

##Implementation
Here we will build a simplified version of `AdaBoostClassifier`. In this case,
our classifer, `AdaBoostBinaryClassifier`, will only predict binary outcomes.
The starter code is in the [code](code) folder.

<br>

We're going to be using a spam dataset. It's in the [data](data) folder. You can see the feature names [here](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names).

Here's how you should be able to run your code after you're finished:

```python
from boosting import AdaBoostBinaryClassifier
import numpy as np
from sklearn.cross_validation import train_test_split

data = np.genfromtxt('data/spam.csv', delimiter=',')

y = data[:, -1]
x = data[:, 0:-1]

train_x, test_x, train_y, test_y = train_test_split(x, y)

my_ada = AdaBoostBinaryClassifier(n_estimators=50)
my_ada.fit(train_x, train_y)
print "Accuracy:", my_ada.score(test_x, test_y)
```

1. Take a look at the `__init__` method. You shouldn't need to change anything here. Note how we are creating Decision Trees that are just a stump! (max depth is 1). If we had a bigger max depth, each of our Decision Trees would be overfit.

1. Implement the `_boost` method. This will be doing steps (a)-(d) inside the for loop.

    Because we need many copies of the estimator, the first step is to clone it. This code is given for you.

    In this function `sample_weight` refers to the *wi*'s in the above description of the algorithm.

    You will need to do these steps:

    * Fix the Decision Tree using the weights. You can do this like this: `estimator.fit(x, y, sample_weight=sample_weight)`
    * Calculate the error term (`estimator_error`)
    * Calculate the alphas (`estimator_weight`)
    * Update the weights (`sample_weight`)

2. Implement the `fit` method. This is steps 1 and 2 from above.

    You should have a for loop that calls your `_boost` method `n_estimators` times. Make sure to save all the estimators in `self.estimators_`. You also need to save all the estimator weights in `self.estimator_weight_`.

3. Implement the `predict` method. This is step 3 from above.

    Note that the algorithm considers the predictions to be either -1 or 1. So once you get predictions back from your Decision Trees, change the 0's to -1's.

4. Implement the `score` method.

    This should call the predict method and then calculate the accuracy.

5. Run the example code from above. Compare your results with sklearn's [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). You should get approximately the same accuracy.

6. Make a plot that of the number of estimators vs accuracy. Put two lines on your graph: one for your version of AdaBoost and one for sklearn's. The accuracy should start really bad and get better with the number of estimators.