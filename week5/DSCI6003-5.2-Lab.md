## Bagging

We have already encountered Bagging in the context of the Random Forest model.  Today we will see not only how to do this with other classifiers, but why it works in that case.

We will be using `scikit-learn` models and hand averaging the predictions.  Also we will learn how to debug and test algorithms along the way.

### Exercise

When implementing/building your own algorithms, remember we always want a small but non-trivial dataset to work with.

1. Use `numpy` or `scipy` to generate a (uneven) bimodal distribution with some Gaussian noise.  This will give us a slightly difficult distribution to try to predict.
2. Plot this distribution of points as well as the true underlying distribution.
3. Using an OLS regression with `scikit-learn`, fit a single line to your data.  With a small holdout of data what is the SSE error?
4. Now, (using an OLS regression again), create a Bagged ensemble of 50 models. For the prediction simply average the prediction of each individual regression line.
5. Again with a small holdout, what is the SSE of the ensemble? Has bagging improved the error?
6. Plot each of the 50 fitted lines of the regression on top of your above plot of the underlying data distribution.
7. Now we will use a Decision Tree regressor.  Perform the same steps #3-6.  Is the bagged ensemble of trees give a lower error?  Why do you think this works for the trees?
8. Create a separate plot of the following as a function of X (your single feature) for both the single decision tree as well as the decision tree ensemble:
    * total error
    * bias-squared
    * variance
    * original Gaussian noise

9. What do you notice about the single decision tree compared to the ensemble?
10. Repeat the above plot for a few other classifiers and compare.  Which classifiers does ensembling help? Which ones does ensembling have minimal effect on?