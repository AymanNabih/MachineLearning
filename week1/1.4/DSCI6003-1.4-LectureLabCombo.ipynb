{"nbformat_minor": 0, "cells": [{"source": "#Gradient Descent", "cell_type": "markdown", "metadata": {}}, {"source": "##Gradient Vector", "cell_type": "markdown", "metadata": {}}, {"source": "Consider the funtion $f({\\bf x})$ where ${\\bf x} \\in R^d$. The gradient is the derivative of $f$ w.r.t $\\bf x$:", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\nabla f({\\bf x}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\left[ \\frac{\\partial f}{\\partial x_1} ~~~ \\frac{\\partial f}{\\partial x_2} ~~~ \\cdots ~~~ \\frac{\\partial f}{\\partial x_d} \\right]^T$$", "cell_type": "markdown", "metadata": {}}, {"source": "\nIf $f$ is differentiable and has an extremum at a point ${\\bf x}$ in a region $S$, then the partial derivatives $\\partial f/\\partial x_j$ must be zero at $\\bf x$. These are the components of the gradient of $f$. Thus", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\nabla f({\\bf x}) = {\\bf 0}$$", "cell_type": "markdown", "metadata": {}}, {"source": "The point at which the gradient is zero, is called a stationary point of $f$.", "cell_type": "markdown", "metadata": {}}, {"source": "##Linear Regression Revisited", "cell_type": "markdown", "metadata": {}}, {"source": "Suppose that at discrete points ${\\bf x}_i$, observations $y_i$ of some phenomenon are made, and the results are recorded as a set of ordered pairs:", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\{({\\bf x}_1, y_1), ({\\bf x}_2, y_2), \\cdots, ({\\bf x}_n, y_n)\\}$$", "cell_type": "markdown", "metadata": {}}, {"source": "On the basis of these points, the problem is to make estimations or predictions at points $\\hat{{\\bf x}}$ that are between or beyond the observation points ${\\bf x}_i$. We can try to find the equation of a curve $y=f({\\bf x})$ that closeley fits the data points.", "cell_type": "markdown", "metadata": {}}, {"source": "The simplest curve to fit the data is a hyperplane:", "cell_type": "markdown", "metadata": {}}, {"source": "$$f({\\bf x}) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_d x_d = {\\bf \\beta}^T {\\bf x}$$", "cell_type": "markdown", "metadata": {}}, {"source": "Here ${\\bf \\beta} = [\\beta_0 ~~ \\beta_1 ~~ \\cdots ~~ \\beta_d]^T$, and we've used $x_0 = 1$.", "cell_type": "markdown", "metadata": {}}, {"source": "<img src=\"imgs/least_squares_plane.png\" width=300>", "cell_type": "markdown", "metadata": {}}, {"source": "###Cost Function", "cell_type": "markdown", "metadata": {}}, {"source": "We want to determine the coefficients $\\beta$ that best fit the data in the sense that the sum of the squares of the errors $e_1, e_2, \\cdots, e_n$ is minimal. The distance from the point $({\\bf x}_i, y_i)$ to the plane $f({\\bf x})$ is", "cell_type": "markdown", "metadata": {}}, {"source": "$$e_i = \\left |f({\\bf x}_i) - y_i \\right| = \\left |\\beta^T {\\bf x}_i - y_i \\right|$$", "cell_type": "markdown", "metadata": {}}, {"source": "If we define a vector ${\\bf e} = [e_1 ~~ e_2 ~~ \\cdots ~~ e_d]^T$, we can write the __cost function__ as", "cell_type": "markdown", "metadata": {}}, {"source": "$$J({\\bf \\beta}) = {\\bf e}^T {\\bf e} = \\sum_{i=1}^{n} e_i^2 = \\sum_{i=1}^{n} (\\beta^T {\\bf x}_i - y_i)^2$$", "cell_type": "markdown", "metadata": {}}, {"source": "We wish to find the the coefficients $\\bf \\beta$ that minimize the above const function. The condition for minima is", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\nabla J({\\bf \\beta}) = \\frac{\\partial J}{\\partial {\\bf \\beta}} = {\\bf 0}$$", "cell_type": "markdown", "metadata": {}}, {"source": "###Gradient Descent", "cell_type": "markdown", "metadata": {}}, {"source": "We can use gradient descent method to find $\\bf \\beta$ that minimizes the cost function. This is an uncontrained optimization of $J({\\bf \\beta})$.", "cell_type": "markdown", "metadata": {}}, {"source": "Suppose that $J$ has a minimum at ${\\hat{\\bf \\beta}}$. We start at some $\\bf \\beta$ (close to $\\hat{\\bf \\beta}$) and we look for the minimum of $J$ close to $\\bf \\beta$ along the straight line in the direction of $-{\\bf \\nabla} J({\\bf \\beta})$, which is the direction of steepest descent (= direction of maximum decrease) of $J$ at $\\bf \\beta$:", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf \\beta}'(\\alpha) = {\\bf \\beta} - \\alpha {\\bf \\nabla} J({\\bf \\beta})$$", "cell_type": "markdown", "metadata": {}}, {"source": "We take ${\\bf \\beta}'$ as our next approximation to ${\\bf \\beta}$. $\\alpha$ is called the _learning rate_.", "cell_type": "markdown", "metadata": {}}, {"source": "To implement the gradient descent method, we first need to compute the partial derivatives fo $J$ w.r.t to components of $\\bf \\beta$. The derivative with respect to the $j$th component of $\\bf \\beta$ can be evaluated as", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\frac{\\partial J}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j}\\left( \\sum_{i=1}^{n} (\\beta^T {\\bf x}_i - y_i)^2 \\right)$$", "cell_type": "markdown", "metadata": {}}, {"source": "Since the derivative is a linear operation, we can write", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_j}(\\beta^T {\\bf x}_i - y_i)^2~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$", "cell_type": "markdown", "metadata": {}}, {"source": "$$~~~~~~~~~~~~~~~~~~~~~~ = \\sum_{i=1}^{n} 2 (\\beta^T {\\bf x}_i - y_i) \\frac{\\partial}{\\partial \\beta_j}(\\beta_0 x_{i0} + \\beta_1 x_{i1} +\\cdots +\\beta_d x_{id} - y_i)$$", "cell_type": "markdown", "metadata": {}}, {"source": "$$ = \\sum_{i=1}^{n} 2 (\\beta^T {\\bf x}_i - y_i) x_{ij}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$", "cell_type": "markdown", "metadata": {}}, {"source": "We can express the above derivative in matrix form by represent the data in terms of the following matrix", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf X} = \\begin{bmatrix} {\\bf x}_1^T \\\\ {\\bf x}_2^T \\\\ \\vdots \\\\ {\\bf x}_n^T \\end{bmatrix} = \\begin{bmatrix} {\\bf X}_0 & {\\bf X}_1 & \\cdots & {\\bf X}_d \\end{bmatrix}$$", "cell_type": "markdown", "metadata": {}}, {"source": "Here ${\\bf X}_j$ denote _feature vectors_. Thus", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf X \\beta} - {\\bf y} = \\begin{bmatrix} {\\bf x}_1^T {\\bf \\beta} - y_1 \\\\ {\\bf x}_2^T {\\bf \\beta} - y_2 \\\\ \\vdots \\\\ {\\bf x}_n^T {\\bf \\beta} - y_n\\end{bmatrix} $$", "cell_type": "markdown", "metadata": {}}, {"source": "Note that ${\\bf \\beta}^T {\\bf x}_i = {\\bf x}_i^T {\\bf \\beta}$. This allows us to express the derivatives of $J$ in more compact form", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\frac{\\partial J}{\\partial \\beta_j} = 2 {\\bf X}_j^T ({\\bf X \\beta} - {\\bf y})$$", "cell_type": "markdown", "metadata": {}}, {"source": "And, finally", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\nabla J({\\bf \\beta}) = \\left[ \\frac{\\partial J}{\\partial \\beta_0} ~~ \\frac{\\partial J}{\\partial \\beta_1} ~~ \\cdots ~~ \\frac{\\partial J}{\\partial \\beta_d} \\right]^T$$", "cell_type": "markdown", "metadata": {}}, {"source": "###Exercise", "cell_type": "markdown", "metadata": {}}, {"source": "We'll consider the data collected in an observational study in a semiconductor manufacturing plant. The file `pull_strength.csv` contains three variables, pull strength (a measure of the amount of force required to break the bond), the wire length, and the height of the die. Find a model relating pull strength to wire length and die height. Plot the data and the model.", "cell_type": "markdown", "metadata": {}}, {"source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n%matplotlib inline\n\ndata = np.genfromtxt('pull_strength.csv', delimiter=',', skip_header=1)\n\ny = data[:,0:1]\nX = data[:,1:3]\n\nA = np.hstack((np.ones([y.shape[0],1]), X))\n\nb = np.linalg.inv(A.T.dot(A)).dot(A.T.dot(y))\nprint 'beta:\\n', b\n\ne = A.dot(b) - y #error vector\nprint \"\\nRSS: \", e.T.dot(e)[0][0]", "cell_type": "code", "execution_count": 1, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "beta:\n[[ 2.26379143]\n [ 2.74426964]\n [ 0.01252781]]\n\nRSS:  115.17348278"}, "execution_count": 1, "metadata": {}}], "metadata": {"trusted": false}}, {"source": "No we can solve the same problem using gradient descent!", "cell_type": "markdown", "metadata": {}}, {"source": "b0 = np.array([[2],[2],[0]])\nh = 0.00001\n\ndef derivative_J(b, j):\n    pass\n\ndef gradient_J(b):\n    pass", "cell_type": "code", "execution_count": 2, "outputs": [], "metadata": {"trusted": false}}, {"source": "###Stochastic Gradient Descent", "cell_type": "markdown", "metadata": {}}, {"source": "The gradient descent method discussed above is called _batch gradient descent_ because it iterates through all the data before calculating $\\bf \\beta$:", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha \\sum_{i=1}^{n} (\\beta^T {\\bf x}_i - y_i) x_{ij}$$", "cell_type": "markdown", "metadata": {}}, {"source": "For larger data sets, this can take very long to converge.", "cell_type": "markdown", "metadata": {}}, {"source": "Alternatively, we can compute the gradient for each iteration on the basis of single randomly picked examle ${\\bf x}_i$:", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha (\\beta^T {\\bf x}_i - y_i) x_{ij}$$", "cell_type": "markdown", "metadata": {}}, {"source": "In general stochastic gradient descent converges to minimum much faster than the batch gradient method.", "cell_type": "markdown", "metadata": {}}, {"source": "###Exercise", "cell_type": "markdown", "metadata": {}}, {"source": "Implement stochastic gradient method for the previous example. Since, we have very few points, we'll note pick example randomly. But still update $\\beta$ for each data point ${\\bf x}_i$.", "cell_type": "markdown", "metadata": {}}, {"source": "##General Case", "cell_type": "markdown", "metadata": {}}, {"source": "In several machine learning algrithms, we wish to learn a hypothesis function, $h({\\bf x})$ given the data set $\\{ {\\bf x}_i\\}$. For linear regression, the hypothesis function is $h = {\\bf \\beta}^T {\\bf x}$. The parameters $\\bf \\beta$ are _learned_ by optimizing the cost function:", "cell_type": "markdown", "metadata": {}}, {"source": "$$J({\\bf \\beta}) = \\sum_{i=1}^{n} (h({\\bf x}_i) - y_i)^2$$", "cell_type": "markdown", "metadata": {}}, {"source": "We can find $\\beta$ by optimizing the cost function, using batch gradient descent method:", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha \\sum_{i=1}^{n} \\nabla_j \\left( h ({\\bf x}_i) - y_i \\right)^2$$", "cell_type": "markdown", "metadata": {}}, {"source": "Here $\\nabla_j$ represents the derivative w.r.t to $\\beta_j$.", "cell_type": "markdown", "metadata": {}}, {"source": "For larger data sets we can use stochastic gradient method instead:", "cell_type": "markdown", "metadata": {}}, {"source": "$${\\bf \\beta}_j' = {\\bf \\beta}_j - \\alpha  \\nabla_j \\left( h ({\\bf x}_i) - y_i \\right)^2$$", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {}}
