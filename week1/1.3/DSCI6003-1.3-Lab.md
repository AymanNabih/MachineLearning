We will be performing a PCA on the classic MNIST dataset of handwritten digits.

### Explore the data

Import the dataset and use `pylab` to explore.

```python
>>> from sklearn.datasets import load_digits
>>> digits = load_digits()
...
>>> import pylab as pl
>>> %pylab inline
>>> pl.gray();
Populating the interactive namespace from numpy and matplotlib
```

    
See [load_digits documentation.](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)
    
Using one figure with 100 subplots in 10-by-10 grid,  display the first 100 images using `pl.imshow`.

![100digits](imgs/first_100_digits.png)

    
To display only the images, use `pl.xticks([]), pl.yticks([])` and  `pl.axis('off')`.
    
#### PCA on subset

For simplicity we will look at the first 6 digits.

1- Load the first 6 digits of the MNIST digits dataset using [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html).

2- For PCA, it is important that our data is normalized/scaled.  Using scikit-learn's [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), scale the digits dataset.

3- Now that we have properly scaled images, we can apply the PCA transformation.  Using scikit-learn's [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), project our digits dataset into lower dimensional space.  First try 10 components.

4- Due to the loss of information in projecting data into lower dimensional space, our transformation is never perfect. One way we can determine how well it worked is to plot the amount of explained variance.  Using the function snippet below, plot the amount of explained variance of each of the principle components.

```python
>>> def scree_plot(num_components, pca, title=None):
...     ind = np.arange(num_components)
...     vals = pca.explained_variance_ratio_
...     plt.figure(figsize=(10, 6), dpi=250)
...     ax = plt.subplot(111)
...     ax.bar(ind, vals, 0.35,
...            color=[(0.949, 0.718, 0.004),
...                   (0.898, 0.49, 0.016),
...                   (0.863, 0, 0.188),
...                   (0.694, 0, 0.345),
...                   (0.486, 0.216, 0.541),
...                   (0.204, 0.396, 0.667),
...                   (0.035, 0.635, 0.459),
...                   (0.486, 0.722, 0.329),
...                  ])
...
...     for i in xrange(num_components):
...             ax.annotate(r"%s%%" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]),
...                         va="bottom", ha="center", fontsize=12)
...     ax.set_xticklabels(ind, fontsize=12)
...
...     ax.set_ylim(0, max(vals)+0.05)
...     ax.set_xlim(0-0.45, 8+0.45)
...
...     ax.xaxis.set_tick_params(width=0)
...     ax.yaxis.set_tick_params(width=2, length=12)
...
...     ax.set_xlabel("Principal Component", fontsize=12)
...     ax.set_ylabel("Variance Explained (%)", fontsize=12)
...
...     if title is not None:
...         plt.title(title, fontsize=16)
```

5- We need to pick an appropriate number of components to keep.  Looking at the plot of explained variance, we are interested in finding the least number of principle components that explain the most variance. What is the optimal number of components to keep for the digits dataset?
    
Another way to visualize our digits is to force a projection into 2-dimensional space in order to visualize the data on a 2-dimensional plane. The code snippet below will plot our digits projected into 2-dimensions on x-y axis.

```python
>>> # plot projection
...
... def plot_embedding(X, y, title=None):
...     x_min, x_max = np.min(X, 0), np.max(X, 0)
...     X = (X - x_min) / (x_max - x_min)
...
...     plt.figure(figsize=(10, 6), dpi=250)
...     ax = plt.subplot(111)
...     ax.axis('off')
...     ax.patch.set_visible(False)
...     for i in range(X.shape[0]):
...         plt.text(X[i, 0], X[i, 1], str(y[i]), color=plt.cm.Set1(y[i] / 10.), fontdict={'weight': 'bold', 'size': 12})
...
...     plt.xticks([]), plt.yticks([])
...     plt.ylim([-0.1,1.1])
...     plt.xlim([-0.1,1.1])
...
...     if title is not None:
...         plt.title(title, fontsize=16)
```

6- Using the above method, project the digits dataset into 2-dimensions.  Do you notice anything about the resulting projections?  Does the plot remind you of anything?  Looking at the results, which digits end up near each other in 2-dimensional space?  Which digits have overlap in this new feature space?
