#Dimensionality Reduction

##Curse of Dimensionality

[Wikipedia] The curse of dimensionality (Bellman, 1961) refers to various phenomena that arise when analyzing and organizing data in high dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.

###Why care about curse of dimensionality

###Volume in Hypersapce

Considere a hypercube with sides $l$. We wish to compute volume occupied by a small margin as shown in the following figure:

<img src="imgs/square_margin.png" width=250/>

In two dimensions, the _volume_ of the margin is

$$V_M = l^2 - (0.9 l)^2 = l^2 (1 - 0.9^2) = 0.19 l^2$$

In three dimension, the volume of the margin becomes

$$V_M = l^3 - (0.9 l)^3 = l^3 (1 - 0.9^3)= 0.27 l^3$$

Fort the general case, as the dimensionality $d$ increases, the volume of the margine becomes:

$$V_M = l^d (1 - 0.9^d) \approx l^d$$

For example, at $d=50$, $99\%$ of the tolal volume is occupied by the margin!

###Python Example

###1-d case

```python
>>> # prep material
... import matplotlib.pyplot as plt
>>> %matplotlib inline
```

```python
>>> # set the dimension and edge length
... d = 1
>>> edgeLen = 1
>>> marginRatio = .1
...
>>> TotVolume = edgeLen**d
...
>>> HoleLen = edgeLen*(1-marginRatio)
...
>>> HoleVolume = HoleLen**d
...
>>> print "When dimension = " + str(d) + " and the margins are " + str(marginRatio*100) + "% of the total edge length:"
>>> print "   Total volume = " + str(TotVolume)
>>> print "   Hole volume = " + str(HoleVolume)
>>> print "   So as a ratio this means " + str(100*(TotVolume - HoleVolume)/TotVolume) + "% of the volume is in the margins."
When dimension = 1 and the margins are 10.0% of the total edge length:
   Total volume = 1
   Hole volume = 0.9
   So as a ratio this means 10.0% of the volume is in the margins.
```

###Generalize for d > 1
Lets build a custom function to do this for arbitrary of dimensions:

```python
>>> def hyperMarginRatio(d = 1, marginRatio = .1, edgeLen = 1):
...     TotVolume = edgeLen**d
...
...     HoleLen = edgeLen*(1-marginRatio)
...     HoleVolume = HoleLen**d
...
...     marginRatio = (TotVolume - HoleVolume)/TotVolume
...
...     print "When dimension = " + str(d) + " and the margins are " + str(marginRatio*100) + "% of the total edge length:"
...     print "   Total volume = " + str(TotVolume)
...     print "   Hole volume = " + str(HoleVolume)
...     print "   So as a ratio,"
...     print str(100*marginRatio) + "% of the volume is in the margins."
...     print ""
...     return marginRatio
```

### 2-dim case

```python
>>> d2 = hyperMarginRatio(d = 2)
When dimension = 2 and the margins are 19.0% of the total edge length:
   Total volume = 1
   Hole volume = 0.81
   So as a ratio,
19.0% of the volume is in the margins.
```

###n-dim case

So for the 2 dimension case, we see that around 19% is in the margins.
Now lets loop through n = 1,2,3,... to see how this ratio increases with dimension number:

```python
>>> maxD = 50
>>> marginRatio = .05
...
...
>>> marginRatios = []
>>> X = range(1,maxD+1)
...
>>> for d in X:
...     appenders = round(hyperMarginRatio(d, marginRatio = marginRatio), 2)
...     marginRatios.append(appenders)
...
>>> print marginRatios
```

```python
>>> plt.plot(X, marginRatios)
>>> plt.title('Ratio of margins volume as a function of dimension')
```

###Distance in Hyperspace

Another practicle effect of high dimensionality is that the distiance between _nearest neighbours_ becomes very large. One the observe this effect through a simple simulation.

Consider a $d$-dimensional _random vector_:

$${\bf x} = [x_1 ~~ x_2 ~~ \cdots ~~ x_d]^T$$

where each component $x_i$ is uniformally distributed between $-1$ and $1$, i.e., $x_i \sim U(-1,1)$. The following figure shows the such vectors $R^3$:

<img src="imgs/uniform_points_cube.gif" width=300/>

We can now measure the distance, $r$, of each point from the origin, and plot a histgram. From this histogram, we can see that the random points can be as close as possible to the origin.

<img src="imgs/distance_3d_hist.gif" width=350/>

As the dimensionality increases, the _smallest_ distance increases. In other words, the closest point to the origin can be very far away. The following two histograms illustrate this effect.

<img src="imgs/distance_50d_hist.gif" width=400/>
<img src="imgs/distance_100d_hist.gif" width=400/>
<img src="imgs/distance_100000d_hist.gif" width=400/>

###Data Sparsity

When the dimensionality $d$ increases, the volume of the space increases so fast that the available data becomes sparse. Consider $N$ uniformaly distributed data points  in $d$ dimensional space. If we construct a hpercube by covering $r$ fraction of range of each fearture, then number of points captured by this hypercube is given by

$$n = N r^d$$

The following simulation illustrates this effect. In this simulation $1000$ points are generated and we're considering $20\%$ of the range of each feature. The small rectangle in $2d$ captures $3.1\%$ of the data points whereas the cube in $3d$ captures only $0.5\%$ of total data points.


<img src="imgs/sparcity_2d.png" width=300/>
<img src="imgs/sparcity_3d.png" width=300/>

####How does this work with binary dimension?

##Dimensionality Reduction

The number of linearly independent basis vectors needed to represent a data vector, gives the dimensionality of the data. e.g.,

$${\bf x} = x_1 {\bf e}_1 + x_2 {\bf e}_2 + \cdots + x_d {\bf e}_d = \sum_{i=1}^{d} x_i {\bf e}_i$$

where ${\bf e}_i$ are orthonormal basis vectors:

$${\bf e}_i^T {\bf e}_j = 1 ~~~~~~~ \text{if} ~~ i = j$$
$$~~~~~~~ = 0 ~~~~~~~ \text{if} ~~ i \ne j$$

Thus the data points apparently reside in a $d$-dimensional _attribute space_. We can represent the data set in terms of a $n\times d$ matrix where $n$ is the number of data points or instances.

$${\bf D} = \begin{bmatrix} {\bf x}_1^T \\ \vdots \\ {\bf x}_n^T \end{bmatrix}$$

Each point ${\bf x}_i^T = [x_{i1} ~~ x_{i2} ~~ \cdots ~~ x_{id}]$ is a vector in $d$-dimensional vector space.

<img src="imgs/iris_data_standard_space.png" width=200 />
$$\text{Iris data in standard feature space.}$$

Given any other set of $d$ orthonormal vectors, we can express ${\bf x}$ as

$${\bf x} = \sum_{i=1}^{d} a_i {\bf u}_i$$

where ${\bf u}_i$ are orthonormal basis vectors:

$${\bf u}_i^T {\bf u}_j = 1 ~~~~~~~ \text{if} ~~ i = j$$
$$~~~~~~~~ = 0 ~~~~~~~ \text{if} ~~ i \ne j$$

<img src="imgs/iris_data_rotated_space.png" width=220 />
$$\text{Iris data in rotated space.}$$

The transformed data matrix can be expressed as

$${\bf A} = \begin{bmatrix} {\bf a}_1^T \\ \vdots \\ {\bf a}_n^T \end{bmatrix}$$

The new representation for each data point is obtained through a linear transformation ${\bf a}_i = {\bf U}^T {\bf x}_i$, where $\bf U$ is the matrix formed by orthonormal basis vectors: ${\bf U} = [{\bf u}_1 ~~ \cdots ~~ {\bf u}_d]$.

Because there are potentially infinite choices for the set of orthonormal basis vectors, one natural question is whether there exists an optimal basis, for a suitable notion of optimality. And, can we find a reduced dimensionality subspace that still preserves the essential characteristics of the data.

Basic idea is to project data points from a $d$-dimensional space to an $r$-dimensional space where $r < d$.

###Principal Component Analysis

Principal Component Analysis (PCA) is a technique that seeks a $r$-dimensional basis that best captures the variance in the data.

The direction with the largest projected variance is called the first principal component. The orthogonal direction that captures the second largest projected variance is called the second principal component, and so on.

<img src="imgs/PCA.jpeg" width=250/>

The projected variance along a direction $\bf u$ is:

$$\sigma_{\bf u} = \frac{1}{n}\sum_{i=1}^n (a_i - \mu_{\bf u})^2$$

where $a_i = {\bf u}^T {\bf x}_i$ are the components of ${\bf a}_i$ along $\bf u$.

If we assume that the data $\bf D$ is centered, _i.e._, $\mu = 0$, then its coordinate  along $\bf u$ is $\mu_{\bf u} = 0$. We can then write the above summation in matrix form:

$$\sigma_{\bf u} =  {\bf u}^T\left(\frac{1}{n}{\bf D}^T {\bf D}\right) {\bf u} = {\bf u}^T {\bf \Sigma u}$$

This implies

$${\bf \Sigma u} = \sigma_{\bf u} {\bf u}$$

This is an eigenvalue equation of the covariance matrix ${\bf \Sigma}$.

To maximize projected variance, we maximize the eigenvalue of $\bf \Sigma$. Eigenvector $\bf u$ corresponding to maximum eigenvalue specifies the direction of most variance, and is called the first principal component.

The eigenvalues of $\bf \Sigma$ are non-negative, and we can sort them in decreasing order:

$$\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_d$$

The matrix ${\bf A} = {\bf D U}$ represents the data in a $d$-dimensional space defined by the eigenvectors of the covariance matrix. That is, each data point is given with respect to new basis $\{{\bf u}_1, \cdots, {\bf u}_d\}$.

One can reduce the dimansionality of data by neglecting ${\bf u}_i$ corresponding to smaller values of $\lambda_i$. Thus the reduced data matrix is given be

$${\bf A}_r = {\bf D U}_r, ~~~~~ \text{where}~~ {\bf U}_r = \begin{bmatrix} {\bf u}_1 & \cdots & {\bf u}_r \end{bmatrix} \text{  such that } r < d$$

Often we may not know how many dimensions, $r$, to use for a good approximation. One criteria for choosing $r$ is to compute the fraction of the total variance captured by the first $r$ principal components, computed as

$$f(r) = \frac{\sum_{i=1}^r \lambda_i}{\sum_{i=1}^d \lambda_i} = \frac{var({\bf A}_r)}{var({\bf D})}$$

Given a certain desired variance threshold, say $\alpha$, starting from the first principal component, we keep on adding additional components, and stop at the smallest value $r$, for which $f(r) \ge \alpha$. In other words, we select the fewest number of dimensions such that the subspace spanned by those $r$ dimensions captures at least $\alpha$ fraction of the total variance. In practice, $\alpha$ is usually set to $0.9$ or higher, so that the reduced dataset captures at least $90\%$ of the total variance.

```python
>>> #avg distance from mean with/withou PCA
... #plot eignevalues
```

###Singular Value Decomposition

Condsider the singular value decomposition of the centered data matrix:

$${\bf D} = {\bf L \Delta R}^T$$

where $\bf L$ and $\bf R$ are orthogonal matrices, and $\bf \Delta$ is a diagonal matrix composes of singular values of $\bf D$. We can calculate the product ${\bf D}^T {\bf D}$, commonly refered to as _scatter matrix_:

$${\bf D}^T {\bf D} = ({\bf L \Delta R}^T)^T {\bf L \Delta R}^T$$

Since ${\bf L}$ is an orthogonal matrix, the above expression simplifies to

$${\bf D}^T {\bf D} =  {\bf R} (\Delta^T \Delta) {\bf R}^T =  {\bf R \Delta}^2_d {\bf R}^T$$

where ${\bf \Delta}^2$ is a $d \times d$ matrix:

$${\bf \Delta}_d^2 = \begin{bmatrix} \delta_1^2 & & \\ & \ddots & \\ & & \delta_d^2 \end{bmatrix}$$

From diagonalization ${\bf \Sigma} = {\bf U \Lambda U}^T$, we get the following:

$${\bf D}^T {\bf D} = n {\bf U \Lambda U}^T = {\bf U} (n {\bf \Lambda}){\bf U}^T$$

Comparing the above two decompositoins of ${\bf D}^T {\bf D}$, we have

$${\bf U} = {\bf R}, ~~~~~ \text{and} ~~~~~ {\bf \Delta}_d^2 = n{\bf \Lambda}$$

Since $\bf \Lambda$ is the digonal matrix of eigenvalues of $\bf \Sigma$, the above expression gives $n \lambda_i = n\sigma_i^2 = \delta_i^2$.

Thus for practical purposes, we can use singular value decomposition to obtain $\bf U$ and hence the recuded data matrix ${\bf A}_r$.

```python

```
