#_K_ Nearest  Neighbors Classifier

##Classification Problem

The task of a classification algorithm or _classifier_ is to predict the _label_ or _class_ for a given unlabeled data point. Mathematically, a classifier is a function or model $f$ that predicts the class label  for a given input example $\bf x$, that is

$$\hat{y} = f({\bf x})$$

The value $\hat{y}$ belongs to a set $\{c_1,c_2,...,c_k\}$ and each $c_i$ is a class label.

To build the model we require a set of points with known class labels, which is called a _training set_. Once the model $f$ is known, we can automatically predict the class for any new data point.

####Fisher's Iris Data Set

Following table shows an extract of the Iris dataset; the complete data forms a $150\times 4$ data matrix. Each entity is an Iris flower, and the attributes include _sepal length_, _sepal width_, _petal length_, and _petal width_ in centimeters, and the type or class of the Iris flower.

<img src="imgs/iris_data_table.png" width=350/>

The classifier algorithm will _learn_ from this data and predict the class of new data set with unknown class.

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
...
>>> X = iris.data
>>> y = iris.target
>>> y_name = iris.target_names
...
>>> print X[:5]
>>> print y_name
[[ 5.1  3.5  1.4  0.2]
 [ 4.9  3.   1.4  0.2]
 [ 4.7  3.2  1.3  0.2]
 [ 4.6  3.1  1.5  0.2]
 [ 5.   3.6  1.4  0.2]]
['setosa' 'versicolor' 'virginica']
```

##_K_ Nearest  Neighbors

In _K_ Nearest Neighbour algorithm, we estimate the density, $p({\bf x}|c_i)$, for each class separately from the data, and then make use of Bayes's theorem.

Suppose that we have a data set $\bf D$ comprising of $n$ points in total. Each data point is represented by a $d$-dimensional vector ${\bf x}_i \in R^d$, and belongs to a particular class $c_k$. Let $n_k$ represents the number of points in $\bf D$ that are labeled with $c_k$ so that $\sum_k n_k = n$. For example, the iris data contains three classes with $50$ points in each class.

In order to classify a new point $\bf x$, we draw a sphere centered on $\bf x$ that contains $K$ points. If this sphere contains $K_k$ points from class $c_k$ the probability density associated with each class is

$$p({\bf x} | c_k) = \frac{K_k/n_k}{V} = \frac{K_k}{n_k V}$$

where $V$ is the volume of the sphere.

Similarly, the unconditional probability density if given by

$$p({\bf x}) = \sum_{k} p({\bf x} | c_k) P(c_k)$$

(Note that $p$ represents probability density, whereas $P$ represents probability.) The class probability $P(c_k)$ is given by

$$P(c_k) = \frac{n_k}{n}$$

Thus

$$p({\bf x}) = \sum_{k} \frac{K_k}{n_k V} \frac{n_k}{n} = \frac{K}{nV}$$

We can obtain the posterior probability of class membership using Bayes' theorem:

$$p(c_k|{\bf x}) = \frac{p({\bf x} | c_k) P(c_k)}{p({\bf x})} = \frac{K_k}{K}$$

Finally, the predicted class for $\bf x$ is the one with the highest posterior probability.

```python

```
