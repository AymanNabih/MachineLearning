{"nbformat_minor": 0, "cells": [{"source": "# Evaluation Metrics", "cell_type": "markdown", "metadata": {}}, {"source": "The goal of this lab is to get comfortable with more advanced evaluation metrics and the intricacies of model performance/selection.", "cell_type": "markdown", "metadata": {}}, {"source": "## Pre-work", "cell_type": "markdown", "metadata": {}}, {"source": "* [Data Science for Business (p. 187-232)](resources/cs194-cross-validate.pdf)", "cell_type": "markdown", "metadata": {}}, {"source": "## Goals", "cell_type": "markdown", "metadata": {}}, {"source": "* Name one situation when you would you use:\n  * Arithmetic mean\n  * Geometric mean\n  * Harmonic mean\n* Given two ROC curves, pick the best curve/threshold for the problem at hand\n* When would you use F_beta vs. AUC\n* State one reason why the Youden index is useful and one reason it can be misleading\n* __Exercise:__ Construct a Profit curve to evaluate the precision/recall trade-off", "cell_type": "markdown", "metadata": {}}, {"source": "## Iteration Zero: Review (9:30 - 9:45)", "cell_type": "markdown", "metadata": {}}, {"source": "A classification problem is when we're trying to predict a discrete (categorical) outcome. We'll start with binary classification (i.e., yes/no questions).", "cell_type": "markdown", "metadata": {}}, {"source": "Here are some example questions:", "cell_type": "markdown", "metadata": {}}, {"source": "* Does a patient have cancer?\n* Will a team win the next game?\n* Will the customer buy my product?\n* Will I get the loan?", "cell_type": "markdown", "metadata": {}}, {"source": "In binary classification, we assign labels of 0 and 1 to our data.", "cell_type": "markdown", "metadata": {}}, {"source": "### Logistic Regression", "cell_type": "markdown", "metadata": {}}, {"source": "Let's start by looking at an example. We're going to be using some NFL data. The x axis is the number of touchdowns scored by team over a season and the y axis is whether they lost or won the game indicated by a value of 0 or 1 respectively.", "cell_type": "markdown", "metadata": {}}, {"source": "![NFL data](images/nfl.png)", "cell_type": "markdown", "metadata": {}}, {"source": "So, how do we predict whether we have a win or a loss if we are given a score? Note that we are going to be predicting values between 0 and 1. Close to 0 means we're sure it's in class 0, close to 1 means we're sure it's in class 1, and closer to 0.5 means we don't know.", "cell_type": "markdown", "metadata": {}}, {"source": "If we use linear regression, we will certainly do better than randomly guessing, but it doesn't accurately represent the data:", "cell_type": "markdown", "metadata": {}}, {"source": "![NFL linear regression](images/linefit.png)", "cell_type": "markdown", "metadata": {}}, {"source": "So clearly a line is not the best way to model this data. So we need to find a better curve.", "cell_type": "markdown", "metadata": {}}, {"source": "## Iteration 1: Measuring success (9:45 - 10:00)", "cell_type": "markdown", "metadata": {}}, {"source": "So how do we measure how well our model does? Just like with regression, we need to split the data in a training set and a test set and measure our success based on how well it does on the test set.", "cell_type": "markdown", "metadata": {}}, {"source": "### Accuracy\nThe simplest measure is **accuracy**. This is the number of correct predictions over the total number of predictions. It's the percent you predicted correctly. In `sklearn`, this is what the `score` method calculates.", "cell_type": "markdown", "metadata": {}}, {"source": "### Shortcomings of Accuracy\nAccuracy is often a good first glance measure, but it has many shortcomings. If the classes are unbalanced, accuracy will not measure how well you did at predicting. Say you are trying to predict whether or not an email is spam. Only 2% of emails are in fact spam emails. You could get 98% accuracy by always predicting not spam. This is a great accuracy but a horrible model!", "cell_type": "markdown", "metadata": {}}, {"source": "### Confusion Matrix\nWe can get a better picture our model but looking at the confusion matrix. We get the following four metrics:", "cell_type": "markdown", "metadata": {}}, {"source": "* **True Positives (TP)**: Correct positive predictions\n* **False Positives (FP)**: Incorrect positive predictions (false alarm)\n* **True Negatives (TN)**: Correct negative predictions\n* **False Negatives (FN)**: Incorrect negative predictions (a miss)", "cell_type": "markdown", "metadata": {}}, {"source": "|            | Predicted Yes  | Predicted No   |\n| ---------- | -------------- | -------------- |\n| Actual Yes | True positive  | False negative |\n| Actual No  | False positive | True negative  |", "cell_type": "markdown", "metadata": {}}, {"source": "With logistic regression, we can visualize it as follows:", "cell_type": "markdown", "metadata": {}}, {"source": "![logistic confusion matrix](images/logistic.png)", "cell_type": "markdown", "metadata": {}}, {"source": "### Precision, Recall and F1\nInstead of accuracy, there are some other scores we can calculate:", "cell_type": "markdown", "metadata": {}}, {"source": "* **Precision**: A measure of how good your positive predictions are\n    ```\n    Precison = TP / (TP + FP)\n             = TP / (predicted yes)\n    ```\n* **Recall**: A measure of how well you predict positive cases. Aka *sensitivity*.\n    ```\n    Recall = TP / (TP + FN) \n           = TP / (actual yes)\n    ```\n* **F1 Score**: The harmonic mean of Precision and Recall\n    ```\n    F1 = 2 / (1/Precision + 1/Recall)\n       = 2 * Precision * Recall / (Precision + Recall)\n       = 2TP / (2TP + FN + FP)\n    ```", "cell_type": "markdown", "metadata": {}}, {"source": "Accuracy can also be written in this notation:\n    ```\n    Accuracy = (TP + TN) / (TP + FP + TN + FN)\n    ```", "cell_type": "markdown", "metadata": {}}, {"source": "### Arithmetic vs. Geometric vs. Harmonic means", "cell_type": "markdown", "metadata": {}}, {"source": "![](http://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/MathematicalMeans.svg/640px-MathematicalMeans.svg.png)", "cell_type": "markdown", "metadata": {}}, {"source": "Just like there are many moments, there are many ways to compute a mean (scientists love generalization) and as such each has its own time and place.", "cell_type": "markdown", "metadata": {}}, {"source": "#### Arithmetic", "cell_type": "markdown", "metadata": {}}, {"source": "The arithmetic mean is the \"average\" we all have grown to know and love from elementary school and high school.", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\displaystyle \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}$$", "cell_type": "markdown", "metadata": {}}, {"source": "When to use:", "cell_type": "markdown", "metadata": {}}, {"source": "* Sum of values is significant (ex: grade in class computed from multiple grades on tests)\n* Need a 'typical' value as an aggregate: minimizes sum squared deviations", "cell_type": "markdown", "metadata": {}}, {"source": "#### Geometric Mean", "cell_type": "markdown", "metadata": {}}, {"source": "The geometric mean is quite similar to the arithmetic mean but uses the product of values rather than sum.", "cell_type": "markdown", "metadata": {}}, {"source": "$$\\displaystyle GM = (\\prod_{i=1}^{n}x_{i})^\\frac{1}{n}$$", "cell_type": "markdown", "metadata": {}}, {"source": "When to use:", "cell_type": "markdown", "metadata": {}}, {"source": "* Product of values is significant (ex: year over year return on an investment)\n*", "cell_type": "markdown", "metadata": {}}, {"source": "__NOTE: The arithmetic mean is strictly larger than the geometric mean for non-negative real numbers.__", "cell_type": "markdown", "metadata": {}}, {"source": "#### Harmonic Mean", "cell_type": "markdown", "metadata": {}}, {"source": "The harmonic mean is by far encountered the least often (in usual applications) but when you do come across it however, it can be very useful.  The Harmonic mean is most often used when dealing with *ratios*.", "cell_type": "markdown", "metadata": {}}, {"source": "$$H = \\frac{n}{\\sum_{i=1}^{n}\\frac{1}{x_{i}}} $$", "cell_type": "markdown", "metadata": {}}, {"source": "__It is the reciprocal of the arithmetic mean of the reciprocals...__", "cell_type": "markdown", "metadata": {}}, {"source": "When to use:", "cell_type": "markdown", "metadata": {}}, {"source": "* Dealing with ratios (ex: average of driving speeds)\n* \n* When you need to compute the F1 score ;)", "cell_type": "markdown", "metadata": {}}, {"source": "__NOTE: The Harmonic mean is strictly less than the geometric (arithmetic) mean for non-negative real numbers.__", "cell_type": "markdown", "metadata": {}}, {"source": "### Example One: Students finishing a homework", "cell_type": "markdown", "metadata": {}}, {"source": "You are tasked with figuring out the total completion time of two students to finish a homework assignment.  Stewart can finish this assignment in 4 hours and Mary can finish this assignment in 7 hours.", "cell_type": "markdown", "metadata": {}}, {"source": "> How long in total will it take Stewart and Mary to finish the homework assignment if they work together (assuming perfect collaboration)?", "cell_type": "markdown", "metadata": {}}, {"source": "### Iteration Two: ROC Curves (10:00 - 10:15)", "cell_type": "markdown", "metadata": {}}, {"source": "One of the best ways to evaluate how a classifier performs is an ROC curve. (http://en.wikipedia.org/wiki/Receiver_operating_characteristic)", "cell_type": "markdown", "metadata": {}}, {"source": "![](images/roc_curve.png)", "cell_type": "markdown", "metadata": {}}, {"source": "To understand what is actually happening with an ROC curve, we can create one ourselves.  Here is pseudo code to plot it.", "cell_type": "markdown", "metadata": {}}, {"source": "The `probabilities` are values in (0,1) returned from Logistic Regression. The standard default threshold is 0.5 where 0-0.5 values are interpreted as the negative class and 0.5-1 values are predicted as the positive class.", "cell_type": "markdown", "metadata": {}}, {"source": "The `labels` are the true values.", "cell_type": "markdown", "metadata": {}}, {"source": "```\nfunction ROC_curve(probabilities, labels):\n    Sort instances by their prediction strength (the probabilities)\n    For every instance in increasing order of probability:\n        Set the threshold to be the probability\n        Set everything above the threshold to the positive class\n        Calculate the True Positive Rate (aka sensitivity or recall)\n        Calculate the False Positive Rate (1 - specificity)\n    Return three lists: TPRs, FPRs, thresholds\n```", "cell_type": "markdown", "metadata": {}}, {"source": "Recall that the *true positive rate* is", "cell_type": "markdown", "metadata": {}}, {"source": "```\n number of true positives     number correctly predicted positive\n-------------------------- = -------------------------------------\n number of positive cases           number of positive cases\n```", "cell_type": "markdown", "metadata": {}}, {"source": "and the *false positive rate* is", "cell_type": "markdown", "metadata": {}}, {"source": "```\n number of false positives     number incorrectly predicted positive\n--------------------------- = ---------------------------------------\n  number of negative cases           number of negative cases\n```", "cell_type": "markdown", "metadata": {}}, {"source": "We are going to be implementing the `roc_curve` function.", "cell_type": "markdown", "metadata": {}}, {"source": "Here's some example code that you should be able to use to plot the ROC curve with your function. This uses a fake dataset.", "cell_type": "markdown", "metadata": {}}, {"source": "", "cell_type": "code", "execution_count": 1, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_validation import train_test_split\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           n_clusters_per_class=2, n_samples=1000)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nprobabilities = model.predict_proba(X_test)[:, 1]\n\ntpr, fpr, thresholds = roc_curve(probabilities, y_test)\n\nplt.plot(fpr, tpr)\nplt.xlabel(\"False Positive Rate (1 - Specificity)\")\nplt.ylabel(\"True Positive Rate (Sensitivity, Recall)\")\nplt.title(\"ROC plot of fake data\")\nplt.show()"}, "execution_count": 1, "metadata": {}}], "metadata": {"trusted": false}}, {"source": "### Iteration Three: ROC Curve Implementation (10:15 - 10:30)", "cell_type": "markdown", "metadata": {}}, {"source": "1. Write an ROC curve function to compute the above in `roc_curve.py`.", "cell_type": "markdown", "metadata": {}}, {"source": "    It should take as input the predicted probabilities and the true labels.", "cell_type": "markdown", "metadata": {}}, {"source": "2. Run the above code to verify that it's working correctly. You can also validate your correctness against [scikit-learns built-in function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html).", "cell_type": "markdown", "metadata": {}}, {"source": "3. Let's see how the roc curve looks on a real dataset. We're going to use the FICO Loan dataset. We want to predict whether or not you get approved for a loan of 12% interest rate given the FICO Score, Loan Length and Loan Amount. Here's the code to load the data:", "cell_type": "markdown", "metadata": {}}, {"source": "    ```python\n    import pandas as pd\n    df = pd.read_csv('data/loanf.csv')\n    y = (df['Interest.Rate'] <= 12).values\n    X = df[['FICO.Score', 'Loan.Length', 'Loan.Amount']].values\n    ```\n\n    Make sure to split your data into training and testing using sklearn's [train_test_split()](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html).", "cell_type": "markdown", "metadata": {}}, {"source": "### Iteration Four: Case Study -- Graduate School Admissions (10:30 - 11:00)", "cell_type": "markdown", "metadata": {}}, {"source": "The data we will be using is admission data on Grad school acceptances.", "cell_type": "markdown", "metadata": {}}, {"source": "* `admit`: whether or not the applicant was admitted to grad. school\n* `gpa`: undergraduate GPA\n* `GRE`: score of GRE test\n* `rank`: prestige of undergraduate school (1 is highest prestige, ala Harvard)", "cell_type": "markdown", "metadata": {}}, {"source": "We will use the GPA, GRE, and rank of the applicants to try to predict whether or not they will be accepted into graduate school.", "cell_type": "markdown", "metadata": {}}, {"source": "Before we get to predictions, we should do some data exploration.", "cell_type": "markdown", "metadata": {}}, {"source": "1. Load in the dataset into pandas: `data/grad.csv`.", "cell_type": "markdown", "metadata": {}}, {"source": "2. Use the pandas `describe` method to get some preliminary summary statistics on the data. In particular look at the mean values of the features.", "cell_type": "markdown", "metadata": {}}, {"source": "3. Use the pandas `crosstab` method to see how many applicants from each rank of school were accepted. You should get a dataframe that looks like this:", "cell_type": "markdown", "metadata": {}}, {"source": "    ```\n    rank    1   2   3   4\n    admit\n    0      28  ..  ..  ..\n    1      33  ..  ..  ..\n    ```\n\n    Make a bar plot of the percent of applicants from each rank who were accepted. You can do `.plot(kind=\"bar\")` on a pandas dataframe.", "cell_type": "markdown", "metadata": {}}, {"source": "4. What does the distribution of the GPA and GRE scores look like? Do the distributions differ much?", "cell_type": "markdown", "metadata": {}}, {"source": "    Hint: Use the pandas `hist` method.", "cell_type": "markdown", "metadata": {}}, {"source": "5. One of the issues with classification can be unbalanced classes. What percentage of the data was admitted? Do you think this will be a problem?", "cell_type": "markdown", "metadata": {}}, {"source": "#### Prediction", "cell_type": "markdown", "metadata": {}}, {"source": "Now we're ready to try to fit our data with Logistic Regression.", "cell_type": "markdown", "metadata": {}}, {"source": "We're going to start with statsmodel's implementation of [Logistic Regression](http://statsmodels.sourceforge.net/stable/generated/statsmodels.discrete.discrete_model.Logit.html) and then move onto sklearn's [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).", "cell_type": "markdown", "metadata": {}}, {"source": "1. Use statsmodels to fit a [Logistic Regression](http://statsmodels.sourceforge.net/stable/generated/statsmodels.discrete.discrete_model.Logit.html).", "cell_type": "markdown", "metadata": {}}, {"source": "2. Use the `summary` method to see your results. Look at the p-values for the beta coefficients. We would like these to be significant. Are they?", "cell_type": "markdown", "metadata": {}}, {"source": "3. Once we feel comfortable with our model, we can move on to cross validation. We no longer will need all the output of statsmodels so we can switch to sklearn. Use sklearn's [KFold cross validation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) and [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to calculate the average accuracy, precision and recall.", "cell_type": "markdown", "metadata": {}}, {"source": "    Hint: Use sklearn's implementation of these scores in [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).", "cell_type": "markdown", "metadata": {}}, {"source": "4. The `rank` column is numerical, but as it has 4 buckets, we could also consider it to be categorical. Use panda's [get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.reshape.get_dummies.html) to binarize the column.", "cell_type": "markdown", "metadata": {}}, {"source": "5. Compute the same metrics as above. Does it do better or worse with the rank column binarized?", "cell_type": "markdown", "metadata": {}}, {"source": "    From now on, use the version of the feature matrix that performed the best.", "cell_type": "markdown", "metadata": {}}, {"source": "6. Make a plot of the ROC curve (using your function defined in Part 1).", "cell_type": "markdown", "metadata": {}}, {"source": "7. Is it possible to pick a threshold where TPR > 60% and FPR < 40%? What is the threshold?", "cell_type": "markdown", "metadata": {}}, {"source": "    *Note that even if it appears to be in the middle of the graph it doesn't make the threshold 0.5.*", "cell_type": "markdown", "metadata": {}}, {"source": "8. Say we are using this as a first step in the application process. We want to weed out clearly unqualified candidates, but not reject too many candidates. What might be a good choice of threshold?", "cell_type": "markdown", "metadata": {}}, {"source": "    There isn't a single correct answer, so explain your choice!", "cell_type": "markdown", "metadata": {}}, {"source": "### Iteration Five: Youden Index (11:00 - 11:15)", "cell_type": "markdown", "metadata": {}}, {"source": "Youden's Index (sometimes called J statistic) is similar to the F1 score in that it is a single number that describes the performance of a classifier.", "cell_type": "markdown", "metadata": {}}, {"source": "$$J = Sensitivity + Specificity - 1$$", "cell_type": "markdown", "metadata": {}}, {"source": "The J statistic ranges from 0 to 1:\n* 0 indicating that the classifier does no better than random\n* 1 indicating that the test performed perfectly", "cell_type": "markdown", "metadata": {}}, {"source": "It can be thought of as an improvement on the F1 score since it takes into account all of the cells in a confusion matrix.  It can also be used to find the optimal threshold for a given ROC curve.", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {}}