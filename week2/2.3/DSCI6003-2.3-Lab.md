## Part 1: Data Exploration: Graduate School Admissions

The data we will be using is admission data on Grad school acceptances.

* `admit`: whether or not the applicant was admitted to grad. school
* `gpa`: undergraduate GPA
* `GRE`: score of GRE test
* `rank`: prestige of undergraduate school (1 is highest prestige, ala Harvard)

We will use the GPA, GRE, and rank of the applicants to try to predict whether or not they will be accepted into graduate school.

Before we get to predictions, we should do some data exploration.

1. Load in the dataset into pandas: `data/grad.csv`.

2. Use the pandas `describe` method to get some preliminary summary statistics on the data. In particular look at the mean values of the features.

3. Use the pandas `crosstab` method to see how many applicants from each rank of school were accepted. You should get a dataframe that looks like this:

    ```
    rank    1   2   3   4
    admit
    0      28  ..  ..  ..
    1      33  ..  ..  ..
    ```

    Make a bar plot of the percent of applicants from each rank who were accepted. You can do `.plot(kind="bar")` on a pandas dataframe.

4. What does the distribution of the GPA and GRE scores look like? Do the distributions differ much?

    Hint: Use the pandas `hist` method.


## Part 2: Predicting Graduate School Admissions

Now we're ready to try to fit our data with Logistic Regression.

We're going to start with statsmodel's implementation of [Logistic Regression](http://statsmodels.sourceforge.net/stable/generated/statsmodels.discrete.discrete_model.Logit.html) and then move onto sklearn's [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).

1. Use statsmodels to fit a [Logistic Regression](http://statsmodels.sourceforge.net/stable/generated/statsmodels.discrete.discrete_model.Logit.html).

2. Use the `summary` method to see your results. Look at the p-values for the beta coefficients. We would like these to be significant. Are they?

3. Once we feel comfortable with our model, we can move on to cross validation. We no longer will need all the output of statsmodels so we can switch to sklearn. Use sklearn's [KFold cross validation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) and [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to calculate the average accuracy, precision and recall.

    Hint: Use sklearn's implementation of these scores in [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).

4. The `rank` column is numerical, but as it has 4 buckets, we could also consider it to be categorical. Use panda's [get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.reshape.get_dummies.html) to binarize the column.

5. Compute the same metrics as above. Does it do better or worse with the rank column binarized?


## Part 3: Interpreting the beta coefficients with the Odds Ratio

One thing that is often lost when talking about logistic regression is the idea of the odds ratio, or rather the probabilistic interpretation of the model. For this next part we will get hands on with the odds ratio.

The ***odds ratio*** is defined as the product of the exponential of each coefficient.

![](images/odds_ratio.png)

This is the odds of being admitted over not being admitted.

It tells you how much a one unit increase of a feature corresponds to the odds of being admitted to grad school. And in doing so the coefficients of the logistic regression can be interpreted similarly to the coefficients of linear regression.

1. Fit a Logistic Regression model on all the data. What are the beta coefficients? You should have 3 values.

2. Compute the change in odds ratio from a one unit change in each feature. This is given by this quantity:

    ![](images/odds.png)

3. Write a sentence for each of the three features that sounds like this:

    *Increasing the GPA by 1 point increases the odds by a factor of ??.*

    Make sure you think about each statement. Does it make sense?

4. What change is required to double my chances of admission? Treat each of the features individually.

    e.g. Increasing the GPA by ?? points doubles the odds.

    *Hint: You need to find the value of k in the following equation.*

    ![](images/odds_double.png)

    The log here is the natural log (log base e).

## Part 4: Predicted Probabilities

Now let's actually play with our data to verify what we calculated above with the Odds Ratio.

1. Create a new feature matrix which has four rows. It should have each of the four possible values for the rank and keep the GRE and GPA values fixed. Use the mean value as a reasonable value to fix them at.

2. Use the same fitted model from Part 4 and then use the model's `predict_proba` method to compute the predicted probabilities of this fake feature matrix. Also include the odds ratio (`p/(1-p)`).

    Note that it gives a numpy array with 2 columns. The first column is the probability that it belongs to class 0 and the second is the probability that it belongs to class 1. These will always sum to 0, so with 2 classes, this is redundant. In this case, the second column is the one we should use since it has the probabilities we expect.

    You should end up with something like this:

    ```
    rank: 1, probability: 0.517495, odds: 0.658981
    ```

    Does the ratio of odds equal what you computed above?

3. Make a plot of the rank vs the probability.

4. We're actually looking at the odds ratio, so make a plot of the rank vs the odds ratio (`p/(1-p)`).

5. Since we really care about how a linear change in rank changes the probability by a multiplicative factor, we should do a graph of the rank vs the log of the odds.

    The slope of this line should be the beta coefficient. Eyeball calculating the slope to confirm this.

6. Do the same analysis (#1-5) with the GRE and GPA scores. Each time, create a feature matrix with the other two columns fixed at the mean and every possible value of the column in question.
