{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exam Review\n",
    "##Test Format:\n",
    "- equations and theory on paper...one coding question\n",
    "\n",
    "##Topics:\n",
    "- intro to ml\n",
    "    - Hypothesis, cost functions and optimization\n",
    "- **KNN**\n",
    "    - knowing what happens in high-dim space with the algo itself, and what you can do to make it work better \n",
    "        - using dimensionality reduction\n",
    "        - using cosine similiarity rather than euclidian distance ($\\sum \\sqrt{(x_{ij} - x_{2j})^2}$...even if things are close, in high dim, they'll have large calculated distances b/c the sum will keep growing due to the number of dimensions)\n",
    "    - KNN is susectiable to scaling differences...so certain distances will be really big, and the largest feature will have less of an effect (b/c you'd have to traverse more of it to make a difference)..where as the smaller features will have a larger effect, b/c a smaller move will mean more. \n",
    "        - Think of it like an elipse.  If you're using a euclidian distance, it reads distance in a circle, and your data may not be distributed as such, so it fails there.  *good to scale your data*\n",
    "    - KNN Actually preforms well in high dim spaces with L2 distance, mainly if you have well defined boundries.  If you have fuzzy boundries, then it might fail. Cosine similirity will generally work better than distance though\n",
    "            - b/c cosine similiarity does not measure distance, but rather angles\n",
    "- **Dim-reduction**\n",
    "    - Good for highly dense and highly sparse data-spaces\n",
    "- **Gradient Descent**\n",
    "    - With Linear Regression:\n",
    "        - hypo: $y= \\beta x$\n",
    "        - cost: $cost = (y-\\beta x)^2$\n",
    "        - $\\frac {d}{d\\beta} = 2\\sum (y-\\beta x) $\n",
    "    - With Logistic Regression:\n",
    "        - if we're doing gradient ascent, then we maximize the log-likelihood\n",
    "        - if we're doing gradient descent, then we minimize the cost func (which in this case IS ONLY THE NEGATIVE (INVERSE) OF THE LOG-LIKLIHOOD)\n",
    "        - then we update our beta's with:\n",
    "            - the data dot'd with the errors (given by the cost function(which is how wrong we are for each observation)\n",
    "\n",
    "- Cross Validation:\n",
    "    - Training set (fit the model initially)\n",
    "    - validation/testing ste ( to test your model preformance in the wild)\n",
    "    - You could also use the training set to train a bunch of different models\n",
    "        - and then use the validation set to test each model (fine tune it)\n",
    "            - and then use  A HOLD OUT SET ...b/c we've been tuning based on the results of that validation set, basically optimizing for that little bit of data.\n",
    "            \n",
    "     - K-Fold:\n",
    "         - where you mix up your training and validation data so you're never optimizing to a single set of training/validation data.  Then, you take the average of the errors (from each of the folds)\n",
    "         \n",
    "      - Bias/Variance Tradeoff:\n",
    "          - we want to ultimately minimize the error of the test set, not the training set!\n",
    "          - but also, you want to choose your hyperparams that set a cetain error rate on your test set...\n",
    "              - should ask yourself: how close will the real world data look like my training/test set?  \n",
    "          - Its also good to err for a slightly higher variance (underfit), since it will have a more simple model...which is easier to understand and explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
